#!/usr/bin/env python3
"""
è¯„ä¼°æœåŠ¡ - MCPæœåŠ¡
æä¾›æ¼”ç»ƒè¯„ä¼°ã€æŠ€èƒ½ç”»åƒã€æŠ¥å‘Šç”Ÿæˆç­‰åŠŸèƒ½
"""

import json
import asyncio
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from fastmcp import FastMCP

# åˆ›å»ºMCPæœåŠ¡å®ä¾‹
mcp = FastMCP(name="Evaluate Service")

# è¯„ä¼°æ•°æ®å­˜å‚¨
evaluation_sessions = {}
skill_profiles = {}
evaluation_metrics = {}

@mcp.tool()
async def create_evaluation_session(exercise_id: str, participants: List[str], evaluation_type: str = "comprehensive") -> str:
    """
    åˆ›å»ºè¯„ä¼°ä¼šè¯
    
    Args:
        exercise_id: æ¼”ç»ƒID
        participants: å‚ä¸äººå‘˜åˆ—è¡¨
        evaluation_type: è¯„ä¼°ç±»å‹ (comprehensive/quick/custom)
    
    Returns:
        è¯„ä¼°ä¼šè¯ä¿¡æ¯
    """
    session_id = f"eval_{uuid.uuid4().hex[:8]}"
    
    session_data = {
        "session_id": session_id,
        "exercise_id": exercise_id,
        "participants": participants,
        "evaluation_type": evaluation_type,
        "start_time": datetime.now().isoformat(),
        "status": "active",
        "metrics": {
            "response_time": [],
            "detection_rate": [],
            "mitigation_effectiveness": [],
            "collaboration_score": [],
            "decision_quality": []
        },
        "events": [],
        "skill_assessments": {}
    }
    
    evaluation_sessions[session_id] = session_data
    
    result = {
        "session_id": session_id,
        "status": "created",
        "participants_count": len(participants),
        "evaluation_framework": "NIST Cybersecurity Framework",
        "assessment_areas": [
            "å¨èƒæ£€æµ‹èƒ½åŠ›",
            "äº‹ä»¶å“åº”é€Ÿåº¦", 
            "å†³ç­–è´¨é‡è¯„ä¼°",
            "å›¢é˜Ÿåä½œæ•ˆæœ",
            "æŠ€æœ¯æŠ€èƒ½æ°´å¹³"
        ]
    }
    
    return json.dumps(result, ensure_ascii=False)

@mcp.tool()
async def record_evaluation_event(session_id: str, event_type: str, participant: str, event_data: Dict[str, Any]) -> str:
    """
    è®°å½•è¯„ä¼°äº‹ä»¶
    
    Args:
        session_id: è¯„ä¼°ä¼šè¯ID
        event_type: äº‹ä»¶ç±»å‹ (detection/response/decision/collaboration)
        participant: å‚ä¸è€…
        event_data: äº‹ä»¶æ•°æ®
    
    Returns:
        äº‹ä»¶è®°å½•ç»“æœ
    """
    if session_id not in evaluation_sessions:
        return json.dumps({"error": "æ— æ•ˆçš„è¯„ä¼°ä¼šè¯ID"})
    
    session = evaluation_sessions[session_id]
    
    event = {
        "event_id": f"evt_{uuid.uuid4().hex[:6]}",
        "timestamp": datetime.now().isoformat(),
        "event_type": event_type,
        "participant": participant,
        "data": event_data,
        "score": calculate_event_score(event_type, event_data)
    }
    
    session["events"].append(event)
    
    # æ›´æ–°å®æ—¶æŒ‡æ ‡
    update_session_metrics(session, event)
    
    result = {
        "event_recorded": True,
        "event_id": event["event_id"],
        "participant_score": event["score"],
        "session_events_count": len(session["events"])
    }
    
    return json.dumps(result, ensure_ascii=False)

def calculate_event_score(event_type: str, event_data: Dict[str, Any]) -> float:
    """è®¡ç®—äº‹ä»¶å¾—åˆ†"""
    base_scores = {
        "detection": 8.0,
        "response": 7.5,
        "decision": 8.5,
        "collaboration": 7.0
    }
    
    base_score = base_scores.get(event_type, 6.0)
    
    # æ ¹æ®äº‹ä»¶æ•°æ®è°ƒæ•´åˆ†æ•°
    if "response_time" in event_data:
        response_time = event_data["response_time"]
        if response_time < 60:  # 1åˆ†é’Ÿå†…å“åº”
            base_score += 1.0
        elif response_time < 300:  # 5åˆ†é’Ÿå†…å“åº”
            base_score += 0.5
        elif response_time > 600:  # è¶…è¿‡10åˆ†é’Ÿ
            base_score -= 1.0
    
    if "accuracy" in event_data:
        accuracy = event_data["accuracy"]
        base_score += (accuracy - 0.7) * 5  # åŸºå‡†70%å‡†ç¡®ç‡
    
    return min(max(base_score, 0.0), 10.0)

def update_session_metrics(session: Dict, event: Dict):
    """æ›´æ–°ä¼šè¯æŒ‡æ ‡"""
    event_type = event["event_type"]
    score = event["score"]
    
    if event_type == "detection":
        session["metrics"]["detection_rate"].append(score)
    elif event_type == "response":
        session["metrics"]["response_time"].append(score)
        session["metrics"]["mitigation_effectiveness"].append(score)
    elif event_type == "decision":
        session["metrics"]["decision_quality"].append(score)
    elif event_type == "collaboration":
        session["metrics"]["collaboration_score"].append(score)

@mcp.tool()
async def generate_skill_profile(participant: str, session_id: str) -> str:
    """
    ç”Ÿæˆä¸ªäººæŠ€èƒ½ç”»åƒ
    
    Args:
        participant: å‚ä¸è€…
        session_id: è¯„ä¼°ä¼šè¯ID
    
    Returns:
        æŠ€èƒ½ç”»åƒåˆ†æç»“æœ
    """
    if session_id not in evaluation_sessions:
        return json.dumps({"error": "æ— æ•ˆçš„è¯„ä¼°ä¼šè¯ID"})
    
    session = evaluation_sessions[session_id]
    participant_events = [e for e in session["events"] if e["participant"] == participant]
    
    if not participant_events:
        return json.dumps({"error": "è¯¥å‚ä¸è€…æ— è¯„ä¼°æ•°æ®"})
    
    # è®¡ç®—å„é¡¹æŠ€èƒ½å¾—åˆ†
    skill_scores = {
        "å¨èƒæ£€æµ‹": calculate_skill_score(participant_events, "detection"),
        "äº‹ä»¶å“åº”": calculate_skill_score(participant_events, "response"),
        "å†³ç­–åˆ†æ": calculate_skill_score(participant_events, "decision"),
        "å›¢é˜Ÿåä½œ": calculate_skill_score(participant_events, "collaboration"),
        "æŠ€æœ¯æ“ä½œ": calculate_technical_score(participant_events)
    }
    
    # ç”ŸæˆæŠ€èƒ½ç”»åƒ
    skill_profile = {
        "participant": participant,
        "evaluation_date": datetime.now().isoformat(),
        "overall_score": sum(skill_scores.values()) / len(skill_scores),
        "skill_breakdown": skill_scores,
        "strengths": identify_strengths(skill_scores),
        "improvement_areas": identify_weaknesses(skill_scores),
        "recommendations": generate_recommendations(skill_scores),
        "competency_level": determine_competency_level(skill_scores),
        "career_path_suggestions": suggest_career_paths(skill_scores)
    }
    
    # å­˜å‚¨æŠ€èƒ½ç”»åƒ
    skill_profiles[f"{participant}_{session_id}"] = skill_profile
    
    return json.dumps(skill_profile, ensure_ascii=False, indent=2)

def calculate_skill_score(events: List[Dict], skill_type: str) -> float:
    """è®¡ç®—ç‰¹å®šæŠ€èƒ½å¾—åˆ†"""
    relevant_events = [e for e in events if e["event_type"] == skill_type]
    if not relevant_events:
        return 6.0  # é»˜è®¤åˆ†æ•°
    
    scores = [e["score"] for e in relevant_events]
    return sum(scores) / len(scores)

def calculate_technical_score(events: List[Dict]) -> float:
    """è®¡ç®—æŠ€æœ¯æ“ä½œå¾—åˆ†"""
    technical_events = [e for e in events if "technical" in e.get("data", {})]
    if not technical_events:
        return 6.5
    
    scores = [e["score"] for e in technical_events]
    return sum(scores) / len(scores)

def identify_strengths(skill_scores: Dict[str, float]) -> List[str]:
    """è¯†åˆ«ä¼˜åŠ¿æŠ€èƒ½"""
    strengths = []
    for skill, score in skill_scores.items():
        if score >= 8.0:
            strengths.append(skill)
    return strengths

def identify_weaknesses(skill_scores: Dict[str, float]) -> List[str]:
    """è¯†åˆ«éœ€è¦æ”¹è¿›çš„æŠ€èƒ½"""
    weaknesses = []
    for skill, score in skill_scores.items():
        if score < 6.0:
            weaknesses.append(skill)
    return weaknesses

def generate_recommendations(skill_scores: Dict[str, float]) -> List[str]:
    """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
    recommendations = []
    
    if skill_scores.get("å¨èƒæ£€æµ‹", 0) < 7.0:
        recommendations.append("å»ºè®®åŠ å¼ºå¨èƒæƒ…æŠ¥åˆ†æå’Œå¼‚å¸¸è¡Œä¸ºè¯†åˆ«è®­ç»ƒ")
    
    if skill_scores.get("äº‹ä»¶å“åº”", 0) < 7.0:
        recommendations.append("å»ºè®®å‚åŠ åº”æ€¥å“åº”æµç¨‹åŸ¹è®­å’Œå®æˆ˜æ¼”ç»ƒ")
    
    if skill_scores.get("å†³ç­–åˆ†æ", 0) < 7.0:
        recommendations.append("å»ºè®®æå‡é£é™©è¯„ä¼°å’Œå†³ç­–åˆ†æèƒ½åŠ›")
    
    if skill_scores.get("å›¢é˜Ÿåä½œ", 0) < 7.0:
        recommendations.append("å»ºè®®åŠ å¼ºè·¨éƒ¨é—¨æ²Ÿé€šåä½œæŠ€èƒ½")
    
    if skill_scores.get("æŠ€æœ¯æ“ä½œ", 0) < 7.0:
        recommendations.append("å»ºè®®æ·±å…¥å­¦ä¹ å®‰å…¨å·¥å…·å’ŒæŠ€æœ¯æ“ä½œ")
    
    return recommendations

def determine_competency_level(skill_scores: Dict[str, float]) -> str:
    """ç¡®å®šèƒ½åŠ›ç­‰çº§"""
    avg_score = sum(skill_scores.values()) / len(skill_scores)
    
    if avg_score >= 9.0:
        return "ä¸“å®¶çº§"
    elif avg_score >= 8.0:
        return "é«˜çº§"
    elif avg_score >= 7.0:
        return "ä¸­çº§"
    elif avg_score >= 6.0:
        return "åˆçº§"
    else:
        return "å…¥é—¨çº§"

def suggest_career_paths(skill_scores: Dict[str, float]) -> List[str]:
    """å»ºè®®èŒä¸šå‘å±•è·¯å¾„"""
    paths = []
    
    if skill_scores.get("å¨èƒæ£€æµ‹", 0) >= 8.0:
        paths.append("å¨èƒæƒ…æŠ¥åˆ†æå¸ˆ")
    
    if skill_scores.get("äº‹ä»¶å“åº”", 0) >= 8.0:
        paths.append("å®‰å…¨äº‹ä»¶å“åº”ä¸“å®¶")
    
    if skill_scores.get("å†³ç­–åˆ†æ", 0) >= 8.0:
        paths.append("å®‰å…¨æ¶æ„å¸ˆ")
    
    if skill_scores.get("æŠ€æœ¯æ“ä½œ", 0) >= 8.0:
        paths.append("æ¸—é€æµ‹è¯•å·¥ç¨‹å¸ˆ")
    
    if not paths:
        paths.append("ç½‘ç»œå®‰å…¨å·¥ç¨‹å¸ˆ")
    
    return paths

@mcp.tool()
async def generate_evaluation_report(session_id: str, report_type: str = "comprehensive") -> str:
    """
    ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    
    Args:
        session_id: è¯„ä¼°ä¼šè¯ID
        report_type: æŠ¥å‘Šç±»å‹ (comprehensive/summary/executive)
    
    Returns:
        è¯„ä¼°æŠ¥å‘Š
    """
    if session_id not in evaluation_sessions:
        return json.dumps({"error": "æ— æ•ˆçš„è¯„ä¼°ä¼šè¯ID"})
    
    session = evaluation_sessions[session_id]
    
    # è®¡ç®—æ•´ä½“æŒ‡æ ‡
    overall_metrics = calculate_overall_metrics(session)
    
    # ç”Ÿæˆå›¢é˜Ÿåˆ†æ
    team_analysis = generate_team_analysis(session)
    
    # ç”Ÿæˆæ”¹è¿›å»ºè®®
    improvement_recommendations = generate_improvement_recommendations(session)
    
    report = {
        "report_id": f"rpt_{uuid.uuid4().hex[:8]}",
        "session_id": session_id,
        "exercise_id": session["exercise_id"],
        "generation_time": datetime.now().isoformat(),
        "report_type": report_type,
        "executive_summary": {
            "overall_score": overall_metrics["overall_score"],
            "participants_count": len(session["participants"]),
            "events_analyzed": len(session["events"]),
            "key_findings": overall_metrics["key_findings"]
        },
        "detailed_metrics": overall_metrics,
        "team_performance": team_analysis,
        "individual_assessments": generate_individual_assessments(session),
        "improvement_recommendations": improvement_recommendations,
        "compliance_assessment": generate_compliance_assessment(session),
        "next_steps": generate_next_steps(overall_metrics)
    }
    
    return json.dumps(report, ensure_ascii=False, indent=2)

def calculate_overall_metrics(session: Dict) -> Dict:
    """è®¡ç®—æ•´ä½“æŒ‡æ ‡"""
    metrics = session["metrics"]
    
    overall_score = 0
    metric_scores = {}
    
    for metric_name, values in metrics.items():
        if values:
            avg_score = sum(values) / len(values)
            metric_scores[metric_name] = avg_score
            overall_score += avg_score
    
    if metric_scores:
        overall_score = overall_score / len(metric_scores)
    
    key_findings = []
    if overall_score >= 8.0:
        key_findings.append("å›¢é˜Ÿæ•´ä½“è¡¨ç°ä¼˜ç§€ï¼Œå…·å¤‡è¾ƒå¼ºçš„å®‰å…¨é˜²æŠ¤èƒ½åŠ›")
    elif overall_score >= 7.0:
        key_findings.append("å›¢é˜Ÿè¡¨ç°è‰¯å¥½ï¼Œåœ¨æŸäº›æ–¹é¢ä»æœ‰æå‡ç©ºé—´")
    else:
        key_findings.append("å›¢é˜Ÿéœ€è¦åŠ å¼ºå®‰å…¨æŠ€èƒ½åŸ¹è®­å’Œå®æˆ˜æ¼”ç»ƒ")
    
    return {
        "overall_score": overall_score,
        "metric_scores": metric_scores,
        "key_findings": key_findings
    }

def generate_team_analysis(session: Dict) -> Dict:
    """ç”Ÿæˆå›¢é˜Ÿåˆ†æ"""
    participants = session["participants"]
    events = session["events"]
    
    team_stats = {
        "collaboration_effectiveness": 7.5,
        "communication_quality": 8.0,
        "role_distribution": "å‡è¡¡",
        "decision_consensus": 85,
        "knowledge_sharing": 7.8
    }
    
    return team_stats

def generate_individual_assessments(session: Dict) -> Dict:
    """ç”Ÿæˆä¸ªäººè¯„ä¼°æ‘˜è¦"""
    assessments = {}
    
    for participant in session["participants"]:
        participant_events = [e for e in session["events"] if e["participant"] == participant]
        if participant_events:
            avg_score = sum(e["score"] for e in participant_events) / len(participant_events)
            assessments[participant] = {
                "average_score": avg_score,
                "events_count": len(participant_events),
                "performance_level": determine_performance_level(avg_score)
            }
    
    return assessments

def determine_performance_level(score: float) -> str:
    """ç¡®å®šè¡¨ç°ç­‰çº§"""
    if score >= 9.0:
        return "å“è¶Š"
    elif score >= 8.0:
        return "ä¼˜ç§€"
    elif score >= 7.0:
        return "è‰¯å¥½"
    elif score >= 6.0:
        return "åˆæ ¼"
    else:
        return "éœ€è¦æ”¹è¿›"

def generate_improvement_recommendations(session: Dict) -> List[str]:
    """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
    recommendations = [
        "å»ºè®®å®šæœŸå¼€å±•å®‰å…¨æ„è¯†åŸ¹è®­ï¼Œæå‡å…¨å‘˜å®‰å…¨ç´ å…»",
        "åŠ å¼ºå¨èƒæƒ…æŠ¥æ”¶é›†å’Œåˆ†æèƒ½åŠ›å»ºè®¾",
        "å®Œå–„åº”æ€¥å“åº”æµç¨‹ï¼Œç¼©çŸ­äº‹ä»¶å¤„ç½®æ—¶é—´",
        "å»ºç«‹è·¨éƒ¨é—¨åä½œæœºåˆ¶ï¼Œæå‡æ•´ä½“é˜²æŠ¤æ•ˆæœ",
        "å¼•å…¥è‡ªåŠ¨åŒ–å®‰å…¨å·¥å…·ï¼Œæé«˜æ£€æµ‹å’Œå“åº”æ•ˆç‡"
    ]
    
    return recommendations

def generate_compliance_assessment(session: Dict) -> Dict:
    """ç”Ÿæˆåˆè§„è¯„ä¼°"""
    return {
        "framework": "NIST Cybersecurity Framework",
        "compliance_score": 82,
        "areas_assessed": [
            "è¯†åˆ«(Identify)",
            "ä¿æŠ¤(Protect)", 
            "æ£€æµ‹(Detect)",
            "å“åº”(Respond)",
            "æ¢å¤(Recover)"
        ],
        "compliance_gaps": [
            "å¨èƒå»ºæ¨¡æµç¨‹éœ€è¦å®Œå–„",
            "äº‹ä»¶å“åº”è®¡åˆ’éœ€è¦æ›´æ–°"
        ]
    }

def generate_next_steps(metrics: Dict) -> List[str]:
    """ç”Ÿæˆåç»­æ­¥éª¤å»ºè®®"""
    return [
        "åˆ¶å®šä¸ªæ€§åŒ–åŸ¹è®­è®¡åˆ’",
        "å®‰æ’è¿›é˜¶å®‰å…¨æŠ€èƒ½åŸ¹è®­",
        "ç»„ç»‡å®šæœŸå®‰å…¨æ¼”ç»ƒ",
        "å»ºç«‹æŠ€èƒ½è¯„ä¼°è·Ÿè¸ªæœºåˆ¶",
        "å®Œå–„å®‰å…¨ç®¡ç†åˆ¶åº¦"
    ]

if __name__ == "__main__":
    print("ğŸ” è¯„ä¼°æœåŠ¡ (Evaluate Service) æ­£åœ¨å¯åŠ¨...")
    mcp.run(transport="http", port=8005)
